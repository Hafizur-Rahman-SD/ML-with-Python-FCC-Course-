{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPERL5ksrwQ/ECEgF2eZoGM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hafizur-Rahman-SD/ML-with-Python-FCC-Course-/blob/main/Linear_Regression_Health_Costs_Calculator_FCC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RgTYVgoIsd2",
        "outputId": "dfacbacc-81bb-4da9-be51-cb50a18c5aad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        sex   bmi  children smoker     region  expenses\n",
            "age                                                    \n",
            "19   female  27.9         0    yes  southwest  16884.92\n",
            "18     male  33.8         1     no  southeast   1725.55\n",
            "28     male  33.0         3     no  southeast   4449.46\n",
            "33     male  22.7         0     no  northwest  21984.47\n",
            "32     male  28.9         0     no  northwest   3866.86\n",
            "✅ Columns: ['sex', 'bmi', 'children', 'smoker', 'region', 'expenses']\n"
          ]
        }
      ],
      "source": [
        "# Cell 1 — Import libraries and load dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Download dataset (if not already)\n",
        "!wget -q https://cdn.freecodecamp.org/project-data/health-costs/insurance.csv\n",
        "\n",
        "# Load dataset correctly (ignore the extra index column)\n",
        "dataset = pd.read_csv('insurance.csv', index_col=0)\n",
        "\n",
        "print(dataset.head())\n",
        "print(\"✅ Columns:\", dataset.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2 — Data preprocessing\n",
        "# Make all column names lowercase (safety step)\n",
        "dataset.columns = dataset.columns.str.strip().str.lower()\n",
        "\n",
        "# Convert categorical columns to numeric\n",
        "dataset = pd.get_dummies(dataset, columns=['sex', 'smoker', 'region'], drop_first=True)\n",
        "print (dataset.head())\n",
        "\n",
        "# Split into train/test\n",
        "train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
        "test_dataset = dataset.drop(train_dataset.index)\n",
        "\n",
        "# Separate labels\n",
        "train_labels = train_dataset.pop('expenses')\n",
        "test_labels = test_dataset.pop('expenses')\n",
        "\n",
        "# Normalize numeric columns for better performance\n",
        "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
        "normalizer.adapt(np.array(train_dataset))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYMbWy9tJM4c",
        "outputId": "314aceea-61f8-408f-cc29-1dd9c95f8831"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      bmi  children  expenses  sex_male  smoker_yes  region_northwest  \\\n",
            "age                                                                     \n",
            "19   27.9         0  16884.92     False        True             False   \n",
            "18   33.8         1   1725.55      True       False             False   \n",
            "28   33.0         3   4449.46      True       False             False   \n",
            "33   22.7         0  21984.47      True       False              True   \n",
            "32   28.9         0   3866.86      True       False              True   \n",
            "\n",
            "     region_southeast  region_southwest  \n",
            "age                                      \n",
            "19              False              True  \n",
            "18               True             False  \n",
            "28               True             False  \n",
            "33              False             False  \n",
            "32              False             False  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3 — Build linear regression model\n",
        "model = keras.Sequential([\n",
        "    normalizer,\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "    loss='mean_absolute_error',\n",
        "    metrics=['mae', 'mse']\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "txxFAIPkJPqU",
        "outputId": "aa18e43c-bdd4-4396-8d40-e793f33d56bc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ normalization_1 (\u001b[38;5;33mNormalization\u001b[0m) │ (\u001b[38;5;34m1070\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │            \u001b[38;5;34m15\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ normalization_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>) │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1070</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m15\u001b[0m (64.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15</span> (64.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m15\u001b[0m (64.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15</span> (64.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4 — Train model\n",
        "EPOCHS = 150\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset, train_labels,\n",
        "    epochs=EPOCHS,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iU7X4CGqJROt",
        "outputId": "5a32f4fb-1a5c-4488-f1c5-c1247b4d4fba"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4228.5840 - mae: 4228.5840 - mse: 37223100.0000 - val_loss: 4644.2720 - val_mae: 4644.2720 - val_mse: 39444484.0000\n",
            "Epoch 2/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4304.4429 - mae: 4304.4429 - mse: 35785628.0000 - val_loss: 4637.0469 - val_mae: 4637.0469 - val_mse: 39324352.0000\n",
            "Epoch 3/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4518.8242 - mae: 4518.8242 - mse: 40120292.0000 - val_loss: 4629.8052 - val_mae: 4629.8052 - val_mse: 39253756.0000\n",
            "Epoch 4/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4470.8506 - mae: 4470.8506 - mse: 40331884.0000 - val_loss: 4635.8813 - val_mae: 4635.8813 - val_mse: 39344460.0000\n",
            "Epoch 5/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4327.8647 - mae: 4327.8647 - mse: 36106340.0000 - val_loss: 4623.2290 - val_mae: 4623.2290 - val_mse: 39180356.0000\n",
            "Epoch 6/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4285.0732 - mae: 4285.0732 - mse: 36546244.0000 - val_loss: 4628.7964 - val_mae: 4628.7964 - val_mse: 39271228.0000\n",
            "Epoch 7/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4267.2969 - mae: 4267.2969 - mse: 35739548.0000 - val_loss: 4630.4185 - val_mae: 4630.4185 - val_mse: 39259576.0000\n",
            "Epoch 8/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4592.1978 - mae: 4592.1978 - mse: 41465252.0000 - val_loss: 4618.6802 - val_mae: 4618.6797 - val_mse: 39123752.0000\n",
            "Epoch 9/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4369.7769 - mae: 4369.7769 - mse: 39437232.0000 - val_loss: 4622.1460 - val_mae: 4622.1460 - val_mse: 39191720.0000\n",
            "Epoch 10/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4493.5542 - mae: 4493.5542 - mse: 40148840.0000 - val_loss: 4617.3394 - val_mae: 4617.3394 - val_mse: 39113012.0000\n",
            "Epoch 11/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4401.4521 - mae: 4401.4521 - mse: 37883644.0000 - val_loss: 4618.6182 - val_mae: 4618.6182 - val_mse: 39122760.0000\n",
            "Epoch 12/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4320.3477 - mae: 4320.3477 - mse: 38198152.0000 - val_loss: 4617.0103 - val_mae: 4617.0103 - val_mse: 39066660.0000\n",
            "Epoch 13/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4323.6699 - mae: 4323.6699 - mse: 38390004.0000 - val_loss: 4616.5439 - val_mae: 4616.5439 - val_mse: 39093400.0000\n",
            "Epoch 14/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4368.4727 - mae: 4368.4727 - mse: 39778824.0000 - val_loss: 4617.1025 - val_mae: 4617.1025 - val_mse: 39052104.0000\n",
            "Epoch 15/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4336.2764 - mae: 4336.2764 - mse: 37578464.0000 - val_loss: 4604.7529 - val_mae: 4604.7529 - val_mse: 38890884.0000\n",
            "Epoch 16/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4570.9536 - mae: 4570.9536 - mse: 41827852.0000 - val_loss: 4608.6284 - val_mae: 4608.6284 - val_mse: 38978312.0000\n",
            "Epoch 17/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4494.3545 - mae: 4494.3545 - mse: 40139908.0000 - val_loss: 4607.1807 - val_mae: 4607.1807 - val_mse: 38938244.0000\n",
            "Epoch 18/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4308.0776 - mae: 4308.0776 - mse: 36144304.0000 - val_loss: 4608.1167 - val_mae: 4608.1167 - val_mse: 38938780.0000\n",
            "Epoch 19/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4269.1401 - mae: 4269.1401 - mse: 37077904.0000 - val_loss: 4600.1465 - val_mae: 4600.1465 - val_mse: 38793328.0000\n",
            "Epoch 20/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4273.2173 - mae: 4273.2173 - mse: 38099180.0000 - val_loss: 4612.1357 - val_mae: 4612.1357 - val_mse: 38967948.0000\n",
            "Epoch 21/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4215.4961 - mae: 4215.4961 - mse: 35008436.0000 - val_loss: 4602.1157 - val_mae: 4602.1157 - val_mse: 38827384.0000\n",
            "Epoch 22/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4140.7886 - mae: 4140.7886 - mse: 35305988.0000 - val_loss: 4622.0513 - val_mae: 4622.0513 - val_mse: 39081440.0000\n",
            "Epoch 23/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4344.4688 - mae: 4344.4688 - mse: 38314840.0000 - val_loss: 4598.9429 - val_mae: 4598.9429 - val_mse: 38759532.0000\n",
            "Epoch 24/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4193.5405 - mae: 4193.5405 - mse: 33986488.0000 - val_loss: 4594.8818 - val_mae: 4594.8818 - val_mse: 38677976.0000\n",
            "Epoch 25/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4303.5142 - mae: 4303.5142 - mse: 35150888.0000 - val_loss: 4600.2700 - val_mae: 4600.2700 - val_mse: 38753948.0000\n",
            "Epoch 26/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4148.5718 - mae: 4148.5718 - mse: 34238828.0000 - val_loss: 4599.1450 - val_mae: 4599.1450 - val_mse: 38748680.0000\n",
            "Epoch 27/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4232.7764 - mae: 4232.7764 - mse: 33585180.0000 - val_loss: 4598.8887 - val_mae: 4598.8887 - val_mse: 38727652.0000\n",
            "Epoch 28/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4162.4307 - mae: 4162.4307 - mse: 36540328.0000 - val_loss: 4589.6958 - val_mae: 4589.6958 - val_mse: 38569644.0000\n",
            "Epoch 29/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4349.0083 - mae: 4349.0083 - mse: 38792644.0000 - val_loss: 4593.6851 - val_mae: 4593.6851 - val_mse: 38675356.0000\n",
            "Epoch 30/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4182.5283 - mae: 4182.5283 - mse: 35636504.0000 - val_loss: 4593.5825 - val_mae: 4593.5825 - val_mse: 38653220.0000\n",
            "Epoch 31/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4305.6006 - mae: 4305.6006 - mse: 38075492.0000 - val_loss: 4591.9312 - val_mae: 4591.9312 - val_mse: 38600012.0000\n",
            "Epoch 32/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4315.5703 - mae: 4315.5703 - mse: 36186900.0000 - val_loss: 4586.2949 - val_mae: 4586.2949 - val_mse: 38510072.0000\n",
            "Epoch 33/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4546.2495 - mae: 4546.2495 - mse: 43263332.0000 - val_loss: 4587.9224 - val_mae: 4587.9224 - val_mse: 38555704.0000\n",
            "Epoch 34/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4145.6250 - mae: 4145.6250 - mse: 35094248.0000 - val_loss: 4581.4355 - val_mae: 4581.4355 - val_mse: 38448440.0000\n",
            "Epoch 35/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4505.0469 - mae: 4505.0469 - mse: 41402312.0000 - val_loss: 4581.9473 - val_mae: 4581.9473 - val_mse: 38436740.0000\n",
            "Epoch 36/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4240.3721 - mae: 4240.3721 - mse: 37409480.0000 - val_loss: 4587.7554 - val_mae: 4587.7554 - val_mse: 38508320.0000\n",
            "Epoch 37/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4344.7681 - mae: 4344.7681 - mse: 38023008.0000 - val_loss: 4581.1729 - val_mae: 4581.1729 - val_mse: 38389628.0000\n",
            "Epoch 38/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4329.8296 - mae: 4329.8296 - mse: 37055496.0000 - val_loss: 4578.6685 - val_mae: 4578.6685 - val_mse: 38390376.0000\n",
            "Epoch 39/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4404.5645 - mae: 4404.5645 - mse: 39372272.0000 - val_loss: 4578.5903 - val_mae: 4578.5903 - val_mse: 38385500.0000\n",
            "Epoch 40/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4272.3164 - mae: 4272.3164 - mse: 38267132.0000 - val_loss: 4580.1318 - val_mae: 4580.1318 - val_mse: 38402060.0000\n",
            "Epoch 41/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4591.9136 - mae: 4591.9136 - mse: 42795688.0000 - val_loss: 4570.3145 - val_mae: 4570.3145 - val_mse: 38232700.0000\n",
            "Epoch 42/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4304.1431 - mae: 4304.1431 - mse: 37637152.0000 - val_loss: 4577.7480 - val_mae: 4577.7480 - val_mse: 38346360.0000\n",
            "Epoch 43/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4263.6362 - mae: 4263.6362 - mse: 37298884.0000 - val_loss: 4576.7773 - val_mae: 4576.7773 - val_mse: 38349368.0000\n",
            "Epoch 44/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4230.5850 - mae: 4230.5850 - mse: 36869316.0000 - val_loss: 4571.3262 - val_mae: 4571.3262 - val_mse: 38283208.0000\n",
            "Epoch 45/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4324.4233 - mae: 4324.4233 - mse: 37606568.0000 - val_loss: 4569.5103 - val_mae: 4569.5103 - val_mse: 38227624.0000\n",
            "Epoch 46/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4195.7495 - mae: 4195.7495 - mse: 34948404.0000 - val_loss: 4575.9858 - val_mae: 4575.9858 - val_mse: 38339616.0000\n",
            "Epoch 47/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4332.8311 - mae: 4332.8311 - mse: 37585460.0000 - val_loss: 4566.9385 - val_mae: 4566.9385 - val_mse: 38209780.0000\n",
            "Epoch 48/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4368.2568 - mae: 4368.2568 - mse: 38494608.0000 - val_loss: 4566.1689 - val_mae: 4566.1689 - val_mse: 38176780.0000\n",
            "Epoch 49/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4265.1025 - mae: 4265.1025 - mse: 36286000.0000 - val_loss: 4574.1436 - val_mae: 4574.1436 - val_mse: 38309980.0000\n",
            "Epoch 50/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4295.0757 - mae: 4295.0757 - mse: 37849748.0000 - val_loss: 4564.4893 - val_mae: 4564.4893 - val_mse: 38144192.0000\n",
            "Epoch 51/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4272.5469 - mae: 4272.5469 - mse: 37186512.0000 - val_loss: 4566.2954 - val_mae: 4566.2954 - val_mse: 38174152.0000\n",
            "Epoch 52/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4368.5552 - mae: 4368.5552 - mse: 37970740.0000 - val_loss: 4565.4536 - val_mae: 4565.4536 - val_mse: 38168244.0000\n",
            "Epoch 53/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4429.3657 - mae: 4429.3657 - mse: 38389564.0000 - val_loss: 4563.1255 - val_mae: 4563.1255 - val_mse: 38126308.0000\n",
            "Epoch 54/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4286.3735 - mae: 4286.3735 - mse: 37651204.0000 - val_loss: 4561.9395 - val_mae: 4561.9395 - val_mse: 38097760.0000\n",
            "Epoch 55/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4171.0552 - mae: 4171.0552 - mse: 36178040.0000 - val_loss: 4557.8682 - val_mae: 4557.8682 - val_mse: 38019324.0000\n",
            "Epoch 56/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4327.2886 - mae: 4327.2886 - mse: 38465700.0000 - val_loss: 4558.9341 - val_mae: 4558.9341 - val_mse: 38039216.0000\n",
            "Epoch 57/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4224.7915 - mae: 4224.7915 - mse: 34529784.0000 - val_loss: 4562.5303 - val_mae: 4562.5303 - val_mse: 38101484.0000\n",
            "Epoch 58/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4203.5645 - mae: 4203.5645 - mse: 34628284.0000 - val_loss: 4555.7549 - val_mae: 4555.7549 - val_mse: 37987688.0000\n",
            "Epoch 59/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3952.4668 - mae: 3952.4668 - mse: 31430522.0000 - val_loss: 4552.3662 - val_mae: 4552.3662 - val_mse: 37933496.0000\n",
            "Epoch 60/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4276.2944 - mae: 4276.2944 - mse: 36719488.0000 - val_loss: 4553.7935 - val_mae: 4553.7935 - val_mse: 37974632.0000\n",
            "Epoch 61/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4370.2812 - mae: 4370.2812 - mse: 38511592.0000 - val_loss: 4551.0093 - val_mae: 4551.0093 - val_mse: 37949480.0000\n",
            "Epoch 62/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4370.6914 - mae: 4370.6914 - mse: 37968528.0000 - val_loss: 4553.1675 - val_mae: 4553.1675 - val_mse: 37941824.0000\n",
            "Epoch 63/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4139.7681 - mae: 4139.7681 - mse: 34993588.0000 - val_loss: 4557.9951 - val_mae: 4557.9951 - val_mse: 38049580.0000\n",
            "Epoch 64/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4127.4922 - mae: 4127.4922 - mse: 34205156.0000 - val_loss: 4552.3623 - val_mae: 4552.3623 - val_mse: 37955104.0000\n",
            "Epoch 65/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4038.8538 - mae: 4038.8538 - mse: 33119288.0000 - val_loss: 4550.7671 - val_mae: 4550.7671 - val_mse: 37928816.0000\n",
            "Epoch 66/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4419.6533 - mae: 4419.6533 - mse: 40884344.0000 - val_loss: 4550.3379 - val_mae: 4550.3379 - val_mse: 37929460.0000\n",
            "Epoch 67/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4162.8125 - mae: 4162.8125 - mse: 36260012.0000 - val_loss: 4550.0112 - val_mae: 4550.0112 - val_mse: 37949776.0000\n",
            "Epoch 68/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4312.4829 - mae: 4312.4829 - mse: 37123796.0000 - val_loss: 4552.0962 - val_mae: 4552.0962 - val_mse: 37933004.0000\n",
            "Epoch 69/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4435.8359 - mae: 4435.8359 - mse: 41343436.0000 - val_loss: 4546.7002 - val_mae: 4546.7002 - val_mse: 37897724.0000\n",
            "Epoch 70/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4134.6768 - mae: 4134.6768 - mse: 35190732.0000 - val_loss: 4546.5674 - val_mae: 4546.5674 - val_mse: 37863488.0000\n",
            "Epoch 71/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4374.6953 - mae: 4374.6953 - mse: 38332312.0000 - val_loss: 4543.7559 - val_mae: 4543.7559 - val_mse: 37832528.0000\n",
            "Epoch 72/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4167.5186 - mae: 4167.5186 - mse: 35051964.0000 - val_loss: 4547.5796 - val_mae: 4547.5796 - val_mse: 37938632.0000\n",
            "Epoch 73/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4224.3750 - mae: 4224.3750 - mse: 37906076.0000 - val_loss: 4542.2686 - val_mae: 4542.2686 - val_mse: 37816784.0000\n",
            "Epoch 74/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4181.2124 - mae: 4181.2124 - mse: 34681336.0000 - val_loss: 4540.0781 - val_mae: 4540.0781 - val_mse: 37781352.0000\n",
            "Epoch 75/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4102.5435 - mae: 4102.5435 - mse: 33605368.0000 - val_loss: 4547.3403 - val_mae: 4547.3403 - val_mse: 37905336.0000\n",
            "Epoch 76/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4239.9019 - mae: 4239.9019 - mse: 36248288.0000 - val_loss: 4540.3281 - val_mae: 4540.3281 - val_mse: 37775868.0000\n",
            "Epoch 77/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4339.2041 - mae: 4339.2041 - mse: 37607360.0000 - val_loss: 4539.5669 - val_mae: 4539.5669 - val_mse: 37790300.0000\n",
            "Epoch 78/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4230.6279 - mae: 4230.6279 - mse: 36826924.0000 - val_loss: 4540.9536 - val_mae: 4540.9536 - val_mse: 37823660.0000\n",
            "Epoch 79/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4250.6504 - mae: 4250.6504 - mse: 38757640.0000 - val_loss: 4542.6724 - val_mae: 4542.6724 - val_mse: 37838004.0000\n",
            "Epoch 80/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4028.0493 - mae: 4028.0493 - mse: 34753060.0000 - val_loss: 4541.8130 - val_mae: 4541.8130 - val_mse: 37846192.0000\n",
            "Epoch 81/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4062.1526 - mae: 4062.1526 - mse: 32204518.0000 - val_loss: 4536.9414 - val_mae: 4536.9414 - val_mse: 37744716.0000\n",
            "Epoch 82/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4405.0796 - mae: 4405.0796 - mse: 39749104.0000 - val_loss: 4539.1191 - val_mae: 4539.1191 - val_mse: 37784600.0000\n",
            "Epoch 83/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4174.6191 - mae: 4174.6191 - mse: 36399388.0000 - val_loss: 4538.7402 - val_mae: 4538.7407 - val_mse: 37780240.0000\n",
            "Epoch 84/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4257.7026 - mae: 4257.7026 - mse: 37127344.0000 - val_loss: 4538.5088 - val_mae: 4538.5088 - val_mse: 37769584.0000\n",
            "Epoch 85/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4152.6416 - mae: 4152.6416 - mse: 34667932.0000 - val_loss: 4535.4263 - val_mae: 4535.4263 - val_mse: 37724620.0000\n",
            "Epoch 86/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4117.8350 - mae: 4117.8350 - mse: 34953612.0000 - val_loss: 4537.0635 - val_mae: 4537.0635 - val_mse: 37750616.0000\n",
            "Epoch 87/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4248.1885 - mae: 4248.1885 - mse: 35549724.0000 - val_loss: 4535.9395 - val_mae: 4535.9395 - val_mse: 37744080.0000\n",
            "Epoch 88/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4134.1025 - mae: 4134.1025 - mse: 34538188.0000 - val_loss: 4537.9844 - val_mae: 4537.9844 - val_mse: 37806952.0000\n",
            "Epoch 89/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4017.2993 - mae: 4017.2993 - mse: 34035424.0000 - val_loss: 4532.6763 - val_mae: 4532.6763 - val_mse: 37671368.0000\n",
            "Epoch 90/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4342.5605 - mae: 4342.5605 - mse: 37980752.0000 - val_loss: 4531.0601 - val_mae: 4531.0601 - val_mse: 37656168.0000\n",
            "Epoch 91/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4358.9478 - mae: 4358.9478 - mse: 38848212.0000 - val_loss: 4531.8027 - val_mae: 4531.8027 - val_mse: 37689736.0000\n",
            "Epoch 92/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4168.5898 - mae: 4168.5898 - mse: 36685736.0000 - val_loss: 4535.3906 - val_mae: 4535.3906 - val_mse: 37753700.0000\n",
            "Epoch 93/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4199.3154 - mae: 4199.3154 - mse: 36653688.0000 - val_loss: 4523.4492 - val_mae: 4523.4492 - val_mse: 37542968.0000\n",
            "Epoch 94/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4352.2490 - mae: 4352.2490 - mse: 36246496.0000 - val_loss: 4528.1099 - val_mae: 4528.1099 - val_mse: 37625056.0000\n",
            "Epoch 95/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4194.8970 - mae: 4194.8970 - mse: 37082592.0000 - val_loss: 4529.1499 - val_mae: 4529.1499 - val_mse: 37673696.0000\n",
            "Epoch 96/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4497.7866 - mae: 4497.7866 - mse: 41134620.0000 - val_loss: 4525.9717 - val_mae: 4525.9717 - val_mse: 37586764.0000\n",
            "Epoch 97/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4089.0342 - mae: 4089.0342 - mse: 34608760.0000 - val_loss: 4531.9521 - val_mae: 4531.9521 - val_mse: 37726620.0000\n",
            "Epoch 98/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4178.8540 - mae: 4178.8540 - mse: 36377956.0000 - val_loss: 4527.5205 - val_mae: 4527.5205 - val_mse: 37684652.0000\n",
            "Epoch 99/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4311.5327 - mae: 4311.5327 - mse: 37305540.0000 - val_loss: 4527.8027 - val_mae: 4527.8027 - val_mse: 37619300.0000\n",
            "Epoch 100/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4228.8735 - mae: 4228.8735 - mse: 36039648.0000 - val_loss: 4529.1304 - val_mae: 4529.1304 - val_mse: 37689572.0000\n",
            "Epoch 101/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4418.0825 - mae: 4418.0825 - mse: 39673856.0000 - val_loss: 4529.8970 - val_mae: 4529.8970 - val_mse: 37721472.0000\n",
            "Epoch 102/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4166.5513 - mae: 4166.5513 - mse: 38713256.0000 - val_loss: 4524.0142 - val_mae: 4524.0142 - val_mse: 37599692.0000\n",
            "Epoch 103/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4431.8950 - mae: 4431.8950 - mse: 41530156.0000 - val_loss: 4521.0913 - val_mae: 4521.0913 - val_mse: 37534916.0000\n",
            "Epoch 104/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4083.0798 - mae: 4083.0798 - mse: 32430610.0000 - val_loss: 4525.1968 - val_mae: 4525.1968 - val_mse: 37597760.0000\n",
            "Epoch 105/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4042.7246 - mae: 4042.7246 - mse: 33045644.0000 - val_loss: 4519.2432 - val_mae: 4519.2432 - val_mse: 37506252.0000\n",
            "Epoch 106/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4174.6382 - mae: 4174.6382 - mse: 36827320.0000 - val_loss: 4522.3857 - val_mae: 4522.3857 - val_mse: 37582888.0000\n",
            "Epoch 107/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4204.3579 - mae: 4204.3579 - mse: 36759872.0000 - val_loss: 4519.7729 - val_mae: 4519.7734 - val_mse: 37525012.0000\n",
            "Epoch 108/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4127.9062 - mae: 4127.9062 - mse: 36096828.0000 - val_loss: 4518.1157 - val_mae: 4518.1157 - val_mse: 37488552.0000\n",
            "Epoch 109/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4071.3347 - mae: 4071.3347 - mse: 34840828.0000 - val_loss: 4526.0435 - val_mae: 4526.0435 - val_mse: 37643420.0000\n",
            "Epoch 110/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4095.8252 - mae: 4095.8252 - mse: 35314448.0000 - val_loss: 4516.6440 - val_mae: 4516.6440 - val_mse: 37464760.0000\n",
            "Epoch 111/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4279.4614 - mae: 4279.4614 - mse: 38206356.0000 - val_loss: 4515.5527 - val_mae: 4515.5527 - val_mse: 37411984.0000\n",
            "Epoch 112/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4167.4780 - mae: 4167.4780 - mse: 34831080.0000 - val_loss: 4514.4941 - val_mae: 4514.4941 - val_mse: 37420104.0000\n",
            "Epoch 113/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4214.9956 - mae: 4214.9956 - mse: 36233356.0000 - val_loss: 4516.6421 - val_mae: 4516.6421 - val_mse: 37439900.0000\n",
            "Epoch 114/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4081.7363 - mae: 4081.7363 - mse: 33526560.0000 - val_loss: 4514.9282 - val_mae: 4514.9282 - val_mse: 37414380.0000\n",
            "Epoch 115/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4145.1377 - mae: 4145.1377 - mse: 36443408.0000 - val_loss: 4521.1938 - val_mae: 4521.1938 - val_mse: 37562200.0000\n",
            "Epoch 116/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4152.4800 - mae: 4152.4800 - mse: 36213768.0000 - val_loss: 4513.1514 - val_mae: 4513.1514 - val_mse: 37405736.0000\n",
            "Epoch 117/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3956.0132 - mae: 3956.0132 - mse: 31600244.0000 - val_loss: 4518.6455 - val_mae: 4518.6455 - val_mse: 37523584.0000\n",
            "Epoch 118/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4057.4670 - mae: 4057.4670 - mse: 34512312.0000 - val_loss: 4515.8789 - val_mae: 4515.8789 - val_mse: 37419056.0000\n",
            "Epoch 119/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4206.0200 - mae: 4206.0200 - mse: 35939660.0000 - val_loss: 4512.9399 - val_mae: 4512.9399 - val_mse: 37389476.0000\n",
            "Epoch 120/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4153.2495 - mae: 4153.2495 - mse: 34292484.0000 - val_loss: 4513.1367 - val_mae: 4513.1367 - val_mse: 37384796.0000\n",
            "Epoch 121/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4130.6772 - mae: 4130.6772 - mse: 36604808.0000 - val_loss: 4513.0474 - val_mae: 4513.0474 - val_mse: 37378420.0000\n",
            "Epoch 122/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4039.8850 - mae: 4039.8850 - mse: 34439076.0000 - val_loss: 4513.6787 - val_mae: 4513.6787 - val_mse: 37384208.0000\n",
            "Epoch 123/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3954.5994 - mae: 3954.5994 - mse: 32355456.0000 - val_loss: 4518.2729 - val_mae: 4518.2729 - val_mse: 37496092.0000\n",
            "Epoch 124/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4174.0366 - mae: 4174.0366 - mse: 34924488.0000 - val_loss: 4507.1787 - val_mae: 4507.1787 - val_mse: 37297180.0000\n",
            "Epoch 125/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4193.6187 - mae: 4193.6187 - mse: 36180516.0000 - val_loss: 4510.9917 - val_mae: 4510.9917 - val_mse: 37354108.0000\n",
            "Epoch 126/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4203.8462 - mae: 4203.8462 - mse: 36293168.0000 - val_loss: 4508.7324 - val_mae: 4508.7324 - val_mse: 37309700.0000\n",
            "Epoch 127/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4299.1885 - mae: 4299.1885 - mse: 36568732.0000 - val_loss: 4508.3247 - val_mae: 4508.3247 - val_mse: 37276680.0000\n",
            "Epoch 128/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4318.4604 - mae: 4318.4604 - mse: 39149464.0000 - val_loss: 4506.8374 - val_mae: 4506.8374 - val_mse: 37267780.0000\n",
            "Epoch 129/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4105.1597 - mae: 4105.1597 - mse: 34465948.0000 - val_loss: 4513.6426 - val_mae: 4513.6426 - val_mse: 37397424.0000\n",
            "Epoch 130/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4190.7012 - mae: 4190.7012 - mse: 37594732.0000 - val_loss: 4506.8013 - val_mae: 4506.8013 - val_mse: 37285040.0000\n",
            "Epoch 131/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4479.6826 - mae: 4479.6826 - mse: 41729200.0000 - val_loss: 4506.3174 - val_mae: 4506.3174 - val_mse: 37276540.0000\n",
            "Epoch 132/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4028.6973 - mae: 4028.6973 - mse: 32695340.0000 - val_loss: 4509.0322 - val_mae: 4509.0322 - val_mse: 37307752.0000\n",
            "Epoch 133/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4370.9248 - mae: 4370.9248 - mse: 40143656.0000 - val_loss: 4500.4458 - val_mae: 4500.4458 - val_mse: 37149424.0000\n",
            "Epoch 134/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4135.6914 - mae: 4135.6914 - mse: 33910592.0000 - val_loss: 4509.8999 - val_mae: 4509.8999 - val_mse: 37363060.0000\n",
            "Epoch 135/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4047.1316 - mae: 4047.1316 - mse: 32946634.0000 - val_loss: 4500.6660 - val_mae: 4500.6660 - val_mse: 37141588.0000\n",
            "Epoch 136/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4146.7690 - mae: 4146.7690 - mse: 36072112.0000 - val_loss: 4501.4902 - val_mae: 4501.4902 - val_mse: 37198740.0000\n",
            "Epoch 137/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4228.1240 - mae: 4228.1240 - mse: 35479624.0000 - val_loss: 4500.9111 - val_mae: 4500.9111 - val_mse: 37183560.0000\n",
            "Epoch 138/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4285.0874 - mae: 4285.0874 - mse: 38015188.0000 - val_loss: 4497.8403 - val_mae: 4497.8403 - val_mse: 37123904.0000\n",
            "Epoch 139/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3907.4482 - mae: 3907.4482 - mse: 32412638.0000 - val_loss: 4497.7510 - val_mae: 4497.7510 - val_mse: 37120504.0000\n",
            "Epoch 140/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4075.7759 - mae: 4075.7759 - mse: 33432036.0000 - val_loss: 4501.9136 - val_mae: 4501.9136 - val_mse: 37203548.0000\n",
            "Epoch 141/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4080.1624 - mae: 4080.1624 - mse: 32770784.0000 - val_loss: 4497.1738 - val_mae: 4497.1738 - val_mse: 37109364.0000\n",
            "Epoch 142/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4015.5579 - mae: 4015.5579 - mse: 33859760.0000 - val_loss: 4498.3462 - val_mae: 4498.3462 - val_mse: 37114688.0000\n",
            "Epoch 143/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4162.5410 - mae: 4162.5410 - mse: 34637684.0000 - val_loss: 4499.3589 - val_mae: 4499.3589 - val_mse: 37151080.0000\n",
            "Epoch 144/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4399.6157 - mae: 4399.6157 - mse: 39089404.0000 - val_loss: 4501.6221 - val_mae: 4501.6221 - val_mse: 37197696.0000\n",
            "Epoch 145/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3951.8354 - mae: 3951.8354 - mse: 31798656.0000 - val_loss: 4493.5171 - val_mae: 4493.5171 - val_mse: 37053260.0000\n",
            "Epoch 146/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4265.8369 - mae: 4265.8369 - mse: 37898400.0000 - val_loss: 4492.7656 - val_mae: 4492.7656 - val_mse: 37010484.0000\n",
            "Epoch 147/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4127.7397 - mae: 4127.7397 - mse: 35463248.0000 - val_loss: 4491.0381 - val_mae: 4491.0381 - val_mse: 36973444.0000\n",
            "Epoch 148/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4070.4832 - mae: 4070.4832 - mse: 32862824.0000 - val_loss: 4493.6890 - val_mae: 4493.6890 - val_mse: 37054376.0000\n",
            "Epoch 149/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4188.6392 - mae: 4188.6392 - mse: 35206500.0000 - val_loss: 4496.3145 - val_mae: 4496.3145 - val_mse: 37100020.0000\n",
            "Epoch 150/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4137.8184 - mae: 4137.8184 - mse: 33849080.0000 - val_loss: 4494.3389 - val_mae: 4494.3389 - val_mse: 37057400.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Cell 5 — Evaluate model performance and visualize results\n",
        "\n",
        "# Make sure index alignment is correct\n",
        "test_dataset = test_dataset.reset_index(drop=True)\n",
        "test_labels = test_labels.reset_index(drop=True)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, mae, mse = model.evaluate(test_dataset, test_labels, verbose=2)\n",
        "print(\"📊 Testing set Mean Abs Error (MAE): {:5.2f} expenses\".format(mae))\n",
        "\n",
        "# Check if challenge passed\n",
        "if mae < 3500:\n",
        "    print(\"✅ You passed the challenge! Great job 🎉\")\n",
        "else:\n",
        "    print(\"❌ The Mean Abs Error must be less than 3500. Keep trying!\")\n",
        "\n",
        "# Predict expenses using the test dataset\n",
        "test_predictions = model.predict(test_dataset).flatten()\n",
        "\n",
        "# Plot predictions vs true values\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(7,7))\n",
        "plt.scatter(test_labels, test_predictions, alpha=0.7)\n",
        "plt.xlabel('True Values (Expenses)')\n",
        "plt.ylabel('Predicted Values (Expenses)')\n",
        "plt.title('True vs Predicted Health Costs')\n",
        "\n",
        "# Line showing perfect prediction\n",
        "lims = [0, 50000]\n",
        "plt.xlim(lims)\n",
        "plt.ylim(lims)\n",
        "plt.plot(lims, lims, 'r')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "4Hs1Q_3PJS6V",
        "outputId": "f524d72e-5cfb-44ff-8d98-ab7308b925fe"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:160: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "math domain error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3496848609.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"📊 Testing set Mean Abs Error (MAE): {:5.2f} expenses\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmae\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/progbar.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values, finalize)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                 \u001b[0mnumdigits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m                 \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"%\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumdigits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"d/%d\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{count} - {now - self._start:.0f}s\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: math domain error"
          ]
        }
      ]
    }
  ]
}